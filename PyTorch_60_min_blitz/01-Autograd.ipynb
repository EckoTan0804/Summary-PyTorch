{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"01-Autograd.ipynb","provenance":[],"authorship_tag":"ABX9TyNYRKKv7fpfwrFKwY00tjwl"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"CIvup4LKvcuB","colab_type":"text"},"source":["# Autograd\n","\n","The `autograd` package provides automatic differentiation for all operations on Tensors. It is a define-by-run framework, which means that your backprop is defined by how your code is run, and that every single iteration can be different."]},{"cell_type":"markdown","metadata":{"id":"d__r1LjVvsfH","colab_type":"text"},"source":["## Tensor\n","\n","`torch.Tensor`: central class of the package\n","\n"," - If you set its attribute `.requires_grad` as `True`, it starts to track all operations on it. When you finish your computation you can call `.backward()` and have all the gradients computed automatically. The gradient for this tensor will be accumulated into `.grad` attribute.\n","\n"," - To stop a tensor from tracking history, you can call `.detach()` to detach it from the computation history, and to prevent future computation from being tracked.\n","\n"," - To prevent tracking history (and using memory), you can also wrap the code block in `with torch.no_grad():`. (This can be particularly helpful when evaluating a model because the model may have trainable parameters with `requires_grad=True`, but for which we don’t need the gradients.)\n","\n","\n"," `Function`: another important class for autograd implementation\n","\n"," `Tensor` and `Function` are interconnected and build up an acyclic graph, that encodes a complete history of computation. **Each tensor has a `.grad_fn` attribute that references a `Function` that has created the `Tensor`** (except for Tensors created by the user - their `grad_fn is None`).\n","\n"," If you want to compute the derivatives, you can call `.backward()` on a `Tensor`. \n","\n","- If `Tensor` is a scalar (i.e. it holds a one element data), you don’t need to specify any arguments to `backward()`\n","- If it has more elements, you need to specify a `gradient` argument that is a tensor of matching shape."]},{"cell_type":"code","metadata":{"id":"gAPpOu76W38S","colab_type":"code","colab":{}},"source":["import torch"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a_uG9DAqYKHD","colab_type":"text"},"source":["### `grad_fn`"]},{"cell_type":"code","metadata":{"id":"9kH7AHZQW7F7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"e25e8bcb-3388-4713-e82c-71aa36e5253e","executionInfo":{"status":"ok","timestamp":1587717869265,"user_tz":-120,"elapsed":1052,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}}},"source":["x = torch.ones(2, 2, requires_grad=True)\n","x"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 1.],\n","        [1., 1.]], requires_grad=True)"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"Boz7EdWSXAiM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"a8e6c78c-db51-4565-a43c-77526581609e","executionInfo":{"status":"ok","timestamp":1587717880531,"user_tz":-120,"elapsed":1306,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}}},"source":["y = x + 2\n","y"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[3., 3.],\n","        [3., 3.]], grad_fn=<AddBackward0>)"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"gVgqJSn4XDUG","colab_type":"text"},"source":["`y` was created as a result of an operation, so it has a `grad_fn`."]},{"cell_type":"code","metadata":{"id":"5DiyOvotXMgJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"ac45d30f-7bf1-4540-ffdb-e99fce39fa45","executionInfo":{"status":"ok","timestamp":1587717930502,"user_tz":-120,"elapsed":1583,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}}},"source":["y.grad_fn"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<AddBackward0 at 0x7fb801ace6a0>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"hCGjbX_WXPed","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"a61a311a-e25e-4a4c-e98e-66a8b64f2e51","executionInfo":{"status":"ok","timestamp":1587718024256,"user_tz":-120,"elapsed":1592,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}}},"source":["z = y * y * 3\n","out = z.mean()\n","print(z, '\\n', out)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["tensor([[27., 27.],\n","        [27., 27.]], grad_fn=<MulBackward0>) \n"," tensor(27., grad_fn=<MeanBackward0>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tv5YeFotXXQ9","colab_type":"text"},"source":["### `required_grad`\n","\n","`.requires_grad_( ... )` changes an existing Tensor’s `requires_grad` flag in-place. The input flag defaults to `False` if not given."]},{"cell_type":"code","metadata":{"id":"-0Tt9xrYYWOW","colab_type":"code","colab":{}},"source":["a = torch.randn(2, 2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"U3q81aIevyvz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"539c924a-9ce7-452d-970d-76fafe060c09","executionInfo":{"status":"ok","timestamp":1587718584324,"user_tz":-120,"elapsed":515,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}}},"source":["a = ((a * 3) / (a - 1))\n","a.requires_grad"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"HSYbJveCZuHM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"3b50287e-cbdb-4841-bddf-5bc6366a34b9","executionInfo":{"status":"ok","timestamp":1587718605452,"user_tz":-120,"elapsed":1050,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}}},"source":["a.requires_grad_(True)\n","a.requires_grad"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"eUF5YcDFZ0Y2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"476a8026-e3e8-471b-bcd8-54909b706dc6","executionInfo":{"status":"ok","timestamp":1587718627029,"user_tz":-120,"elapsed":997,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}}},"source":["b = (a * a).sum()\n","b.grad_fn"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<SumBackward0 at 0x7fb7b45c79b0>"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"bmi-SrCGZ5jl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"25544a17-c9c1-4651-8286-51ff8c488cdb","executionInfo":{"status":"ok","timestamp":1587718649687,"user_tz":-120,"elapsed":1078,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}}},"source":["b.requires_grad"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"TnBn7O7RZ_Lk","colab_type":"text"},"source":["## Gradients"]},{"cell_type":"code","metadata":{"id":"iqGhVyEjaIzR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"ae3b4889-da02-42ea-fbf8-5cb5f6429753","executionInfo":{"status":"ok","timestamp":1587718723162,"user_tz":-120,"elapsed":849,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}}},"source":["out"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(27., grad_fn=<MeanBackward0>)"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"4mvLm0Y2aRHD","colab_type":"text"},"source":["Because `out` contains a single scalar, `out.backward()` is equivalent to `out.backward(torch.tensor(1.))`."]},{"cell_type":"code","metadata":{"id":"jpEvscr-av6O","colab_type":"code","colab":{}},"source":["out.backward()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"71GdoN2vaYTX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"913a21e3-5b95-4108-dbbb-bdfda5a1fbd6","executionInfo":{"status":"ok","timestamp":1587718857651,"user_tz":-120,"elapsed":560,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}}},"source":["# gradients d(out)/dx\n","print(x.grad)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["tensor([[4.5000, 4.5000],\n","        [4.5000, 4.5000]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3lBM-kQrajr0","colab_type":"text"},"source":["Let's call the `out` Tensor $o$.\n","\n","We have \n","$$\n","o=\\frac{1}{4} \\sum_{i} z_{i} \\\\\n","z_i = 3(x_i + 2)^2\n","$$\n","\n","Therefore:\n","$$\n","\\frac{\\partial o}{\\partial z_i} = \\frac{1}{4} \\\\\n","\\frac{\\partial z_i}{\\partial x_i} = 6(x_i + 2)\n","$$\n","\n","According to **Chain Rule**:\n","$$\n","\\frac{\\partial o}{\\partial x_i} = \\frac{\\partial o}{\\partial z_i} \\frac{\\partial z_i}{\\partial x_i} = \\frac{1}{4} \\cdot 6(x_i + 2) \\overset{x_i = 1}{=} = 4.5\n","$$"]},{"cell_type":"markdown","metadata":{"id":"TqzZKSddc849","colab_type":"text"},"source":["### Vector-Jacobian product\n","\n","Mathematically, if you have a vector valued function $\\vec{y}=f(\\vec{x})$, then the gradient of $\\vec{y}$ with respect to $\\vec{x}$ is a Jacobian matrix:\n","\n","$$\n","J=\\left(\\begin{array}{ccc}\n","\\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}} \\\\\n","\\vdots & \\ddots & \\vdots \\\\\n","\\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n","\\end{array}\\right)\n","$$\n","\n","Generally speaking, `torch.autograd` is an engine for computing vector-Jacobian product. That is, given any vector \n","\n","$$v=\\left(\\begin{array}{llll}v_{1} & v_{2} & \\cdots & v_{m}\\end{array}\\right)^{T}$$\n","\n",", compute the product $v^T \\cdot J$. If $v$ happens to be the gradient of a scalar function $l = g(\\vec{y})$, that is,\n","\n","$$\n","v=\\left(\\begin{array}{lll}\n","\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\n","\\end{array}\\right)^{T}\n","$$\n","\n",", then by the chain rule, the vector-Jacobian product would be the gradient of $l$ with respect to $\\vec{x}$:\n","\n","$$\n","J^{T} \\cdot v=\\left(\\begin{array}{ccc}\n","\\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}} \\\\\n","\\vdots & \\ddots & \\vdots \\\\\n","\\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n","\\end{array}\\right)\\left(\\begin{array}{c}\n","\\frac{\\partial l}{\\partial y_{1}} \\\\\n","\\vdots \\\\\n","\\frac{\\partial l}{\\partial y_{m}}\n","\\end{array}\\right)=\\left(\\begin{array}{c}\n","\\frac{\\partial t}{\\partial x_{1}} \\\\\n","\\vdots \\\\\n","\\frac{\\partial l}{\\partial x_{n}}\n","\\end{array}\\right)\n","$$\n","\n","> Note that $v^T \\cdot J$ gives a row vector which can be treated as a column vector by taking $J^T \\cdot v$\n","\n","This characteristic of vector-Jacobian product makes it very convenient to feed external gradients into a model that has non-scalar output.\n"]},{"cell_type":"code","metadata":{"id":"AeVOpqS8gaZL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"7f251926-f109-43ef-a3da-c4228dda826d","executionInfo":{"status":"ok","timestamp":1587720377158,"user_tz":-120,"elapsed":1088,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}}},"source":["x = torch.randn(3, requires_grad=True)\n","\n","y = x * 2\n","while y.data.norm() < 1000:\n","  y = y * 2\n","\n","print(y)"],"execution_count":21,"outputs":[{"output_type":"stream","text":["tensor([-1389.8010,  1174.6008,   706.2604], grad_fn=<MulBackward0>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ROoKAnnHgk4W","colab_type":"text"},"source":["In this case `y` is no longer a scalar. `torch.autograd` could not compute the full Jacobian directly, but if we just want the vector-Jacobian product, simply pass the vector to `backward` as argument:"]},{"cell_type":"code","metadata":{"id":"oPkkVz39g8AN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"11535993-dd63-43ef-c25a-267415839648","executionInfo":{"status":"ok","timestamp":1587720514203,"user_tz":-120,"elapsed":488,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}}},"source":["v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n","y.backward(v)\n","\n","print(x.grad)"],"execution_count":22,"outputs":[{"output_type":"stream","text":["tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"orOQNbQ4ahU5","colab_type":"text"},"source":["## Stop autograd\n","\n","Stop autograd from tracking history on Tensors with `.requires_grad=True` \n","\n","- either by \n","wrapping the code block in `with torch.no_grad():`"]},{"cell_type":"code","metadata":{"id":"nioyc_Y-hrwd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"d726100c-04e6-410d-d327-2f75006f578e","executionInfo":{"status":"ok","timestamp":1587720704216,"user_tz":-120,"elapsed":740,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}}},"source":["print(x.requires_grad)\n","print((x ** 2).requires_grad)"],"execution_count":25,"outputs":[{"output_type":"stream","text":["True\n","True\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"y3IYtY2Bh02h","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"196416f7-c44b-422d-e71c-0d6f2ae00c67","executionInfo":{"status":"ok","timestamp":1587720736731,"user_tz":-120,"elapsed":1244,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}}},"source":["with torch.no_grad():\n","  print((x ** 2).requires_grad)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["False\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pxGAtv8oh8qX","colab_type":"text"},"source":["- Or by using `.detach()` to get a new Tensor with the same content but that does not require gradients:"]},{"cell_type":"code","metadata":{"id":"mHRXq2dwiCnr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"8fb16609-a462-4ed1-ea00-808555ed8945","executionInfo":{"status":"ok","timestamp":1587720800873,"user_tz":-120,"elapsed":845,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}}},"source":["print(x.requires_grad)\n","y = x.detach()\n","print(y.requires_grad)\n","print(x.eq(y).all())"],"execution_count":27,"outputs":[{"output_type":"stream","text":["True\n","False\n","tensor(True)\n"],"name":"stdout"}]}]}