{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"02-Neural_Networks.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN5p34QFjSdi13CA8/v+iTW"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"WpnBPTGrkd60","colab_type":"text"},"source":["# Neural Networks\n","\n","Neural networks can be constructed using the `torch.nn` package. `nn` depends on `autograd` to define models and differentiate them. An `nn.Module` contains layers, and a method `forward(input)`that returns the `output`.\n","\n","Example: \n","\n","![ConvNet](https://pytorch.org/tutorials/_images/mnist.png)\n","\n","It is a simple feed-forward network. It takes the input, feeds it through several layers one after the other, and then finally gives the output."]},{"cell_type":"markdown","metadata":{"id":"799d-UkqlmNL","colab_type":"text"},"source":["A typical training procedure for a neural network is as follows:\n","\n","- Define the neural network that has some learnable parameters (or weights)\n","- Iterate over a dataset of inputs\n","- Process input through the network\n","- Compute the loss (how far is the output from being correct)\n","- Propagate gradients back into the networkâ€™s parameters\n","- Update the weights of the network, typically using a simple update rule: `weight = weight - learning_rate * gradient`"]},{"cell_type":"markdown","metadata":{"id":"QGDUcnP_lygN","colab_type":"text"},"source":["## Define the network"]},{"cell_type":"code","metadata":{"id":"f34_DW64l4d-","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"THW9ZRhLmfQs","colab_type":"code","colab":{}},"source":["class Net(nn.Module):\n","\n","  def __init__(self):\n","    super(Net, self).__init__()\n","    \n","    # 1 input image channel, 6 output channels, 3x3 square convolutiuon\n","\n","    # kernel\n","    self.conv1 = nn.Conv2d(1, 6, 3)\n","    self.conv2 = nn.Conv2d(6, 16, 3) \n","\n","    # affine operation: y = Wx + b\n","    self.fc1 = nn.Linear(16 * 6 * 6, 120) # 6*6 from image dimension\n","    self.fc2 = nn.Linear(120, 84)\n","    self.fc3 = nn.Linear(84, 10)\n","\n","  def forward(self, x):\n","\n","    # Max pooling over a (2, 2) window\n","    x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n","    # If the size is a square you can only specify a single number\n","    x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n","    \n","    x = x.view(-1, self.num_flat_features(x))\n","\n","    x = F.relu(self.fc1(x))\n","    x = F.relu(self.fc2(x))\n","    x = self.fc3(x)\n","\n","    return x\n","\n","  def num_flat_features(self, x):\n","    size = x.size()[1:] # all dimension except the batch dimension\n","    num_features = 1\n","    for s in size:\n","      num_features *= s\n","    return num_features"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bQY5KJ7hn56w","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"9a71e120-d669-493c-a9e4-ac9ef0eddf80","executionInfo":{"status":"ok","timestamp":1587723400848,"user_tz":-120,"elapsed":646,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}}},"source":["net = Net()\n","print(net)"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Net(\n","  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n","  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n","  (fc1): Linear(in_features=576, out_features=120, bias=True)\n","  (fc2): Linear(in_features=120, out_features=84, bias=True)\n","  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",")\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"buLrmJngn78j","colab_type":"text"},"source":["You just have to define the `forward` function, and the `backward` function (where gradients are computed) is automatically defined for you using `autograd` ðŸ‘». You can use any of the Tensor operations in the `forward` function.\n","\n","The learnable parameters of a model are returned by `net.parameters()`:"]},{"cell_type":"code","metadata":{"id":"LxXIaIodoV0p","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"eea80368-79b2-4695-ecdd-944f6950ec6c","executionInfo":{"status":"ok","timestamp":1587723401249,"user_tz":-120,"elapsed":1031,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}}},"source":["params = list(net.parameters())\n","print(len(params))\n","print(params[0].size()) # conv1's weight"],"execution_count":22,"outputs":[{"output_type":"stream","text":["10\n","torch.Size([6, 1, 3, 3])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dg4ieLTUod3F","colab_type":"text"},"source":["Letâ€™s try a random 32x32 input. \n","\n","> Note: expected input size of this net (LeNet) is 32x32. To use this net on the MNIST dataset, please resize the images from the dataset to 32x32."]},{"cell_type":"code","metadata":{"id":"WVmOox0bouCW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"ec74dbfd-b967-468e-893c-8cce4db11edb","executionInfo":{"status":"ok","timestamp":1587723401250,"user_tz":-120,"elapsed":1011,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}}},"source":["input = torch.randn(1, 1, 32, 32) # nSamples x nChannels x Height x Width\n","input"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[[ 0.5477,  1.9696, -1.0139,  ...,  0.0520, -0.5070, -1.5527],\n","          [-0.0891,  1.1150,  0.6102,  ..., -0.5599,  0.7167,  0.8280],\n","          [-1.0423, -0.0944,  0.4863,  ...,  1.5127,  0.8198,  0.6389],\n","          ...,\n","          [ 2.2860, -0.2799,  0.2085,  ...,  0.8444,  0.7534, -0.7493],\n","          [ 0.8204, -1.6026, -0.6270,  ..., -1.1169,  0.5376,  1.1022],\n","          [ 0.1709,  0.7838, -1.2950,  ..., -0.3746, -0.2765,  1.9111]]]])"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"JIldqfp6o5T9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"1a91b8f9-038d-4b5c-cc6e-2ada0527c878","executionInfo":{"status":"ok","timestamp":1587723401251,"user_tz":-120,"elapsed":992,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}}},"source":["out = net(input)\n","print(out)"],"execution_count":24,"outputs":[{"output_type":"stream","text":["tensor([[-0.0322, -0.0178,  0.0594, -0.0036, -0.0501,  0.1093,  0.0533,  0.0458,\n","         -0.0099, -0.1082]], grad_fn=<AddmmBackward>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FhW_iSq5pG28","colab_type":"text"},"source":["Zero the gradient buffers of all parameters and backprops with random gradients:"]},{"cell_type":"code","metadata":{"id":"RpMVyyQipMS_","colab_type":"code","colab":{}},"source":["net.zero_grad()\n","out.backward(torch.randn(1, 10))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"87Mt-Sh8dso0","colab_type":"text"},"source":["> Note: \\\n","> `torch.nn` only supports **mini-batches**. The entire `torch.nn` package only supports inputs that are a mini-batch of samples, and not a single sample.\\\n","For example, `nn.Conv2d` will take in a 4D Tensor of `nSamples x nChannels x Height x Width`.\\\n","If you have a single sample, just use `input.unsqueeze(0)` to add a fake batch dimension."]},{"cell_type":"markdown","metadata":{"id":"QhptvLBHo2w4","colab_type":"text"},"source":["## Loss function\n","\n","A loss function takes the (output, target) pair of inputs, and computes a value that estimates how far away the output is from the target.\n","\n","There are several different [loss functions](https://pytorch.org/docs/nn.html#loss-functions) under the nn package . A simple loss is: `nn.MSELoss` which computes the mean-squared error between the input and the target."]},{"cell_type":"code","metadata":{"id":"MtlCiApqo0Uw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"4fcdb98a-4e34-4e48-92c4-e952923060f7","executionInfo":{"status":"ok","timestamp":1587723401253,"user_tz":-120,"elapsed":954,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}}},"source":["output = net(input)\n","target = torch.randn(10) # a dummy target for example\n","target = target.view(1, -1) # make it the same shape as output\n","criterion = nn.MSELoss()\n","\n","loss = criterion(output, target)\n","loss"],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.9301, grad_fn=<MseLossBackward>)"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"KF5ATKXSqbv0","colab_type":"text"},"source":["Now, if you follow `loss` in the backward direction, using its `.grad_fn` attribute, you will see a graph of computations that looks like this:\n","\n","```\n","input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n","      -> view -> linear -> relu -> linear -> relu -> linear\n","      -> MSELoss\n","      -> loss\n","```\n","\n","So, when we call `loss.backward()`, the whole graph is differentiated w.r.t. the loss, and all Tensors in the graph that has `requires_grad=True` will have their `.grad` Tensor accumulated with the gradient.\n","\n","For illustration, follow a few steps backward:"]},{"cell_type":"code","metadata":{"id":"6pFTB8Yfq1Cm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"79876bf3-cc62-4c81-cc03-8ec2a36cdeb3","executionInfo":{"status":"ok","timestamp":1587723401254,"user_tz":-120,"elapsed":942,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}}},"source":["print(loss.grad_fn)  # MSELoss\n","print(loss.grad_fn.next_functions[0][0])  # Linear\n","print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"],"execution_count":27,"outputs":[{"output_type":"stream","text":["<MseLossBackward object at 0x7f5e903f1080>\n","<AddmmBackward object at 0x7f5e903f1080>\n","<AccumulateGrad object at 0x7f5e90439748>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eVjVytH2rfHk","colab_type":"text"},"source":["## Backprop\n","\n","To backpropagate the error all we have to do is to `loss.backward()`. You need to clear the existing gradients though, else gradients will be accumulated to existing gradients."]},{"cell_type":"code","metadata":{"id":"qsdXq2ZLru8D","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"16941472-230c-46c1-9f91-8db162a65c5a","executionInfo":{"status":"ok","timestamp":1587723401255,"user_tz":-120,"elapsed":920,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}}},"source":["net.zero_grad() # zeroes the gradient buffers of all parameters\n","\n","print('conv1.bias.grad before backward: ')\n","print(net.conv1.bias.grad)\n","\n","loss.backward()\n","\n","print('conv1.bias.grad after backward:')\n","print(net.conv1.bias.grad)"],"execution_count":28,"outputs":[{"output_type":"stream","text":["conv1.bias.grad before backward: \n","tensor([0., 0., 0., 0., 0., 0.])\n","conv1.bias.grad after backward:\n","tensor([-0.0022,  0.0010,  0.0060, -0.0046, -0.0027,  0.0137])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"k6NlOfT_sDWP","colab_type":"text"},"source":["## Update the weights\n","\n","The simplest update rule used in practice is the **Stochastic Gradient Descent (SGD)**:\n","\n","```\n","weight = weight - learning_rate * gradient\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"QwbDmh-lr0xc","colab_type":"text"},"source":["Implement this using simple Python code:"]},{"cell_type":"code","metadata":{"id":"NSQ4-qgksjnc","colab_type":"code","colab":{}},"source":["learning_rate = 0.01\n","for f in net.parameters():\n","  f.data.sub_(f.grad.data * learning_rate)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ud4FBZF2suMq","colab_type":"text"},"source":["However, as you use neural networks, you want to use various different update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc."]},{"cell_type":"code","metadata":{"id":"6fKtOyyctKwU","colab_type":"code","colab":{}},"source":["import torch.optim as optim\n","\n","# Create optimizer\n","optimizer = optim.SGD(net.parameters(), lr=0.01)\n","\n","# in training loop:\n","optimizer.zero_grad() # zero the gradient buffer\n","output = net(input)\n","loss = criterion(output, target)\n","loss.backward()\n","optimizer.step() # update"],"execution_count":0,"outputs":[]}]}